# -*- coding: utf-8 -*-
"""UTSA 2019 Oct - Part2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cyQBC-d2mPA1wVEv5rtHxAPRUHq4-yJW

# Exercise 1 - Loading data

## Download data
"""

!rm -rf utsa-workshop-20191024/
!git clone https://github.com/marco-antonio-villegas/utsa-workshop-20191024



"""## Unzip files"""

import zipfile
with zipfile.ZipFile("utsa-workshop-20191024/data/autos.csv.zip","r") as zipref:
  zipref.extractall("data-folder")
!ls data-folder/

"""## Load Pandas dataframe"""

import pandas as pd
df_autos = pd.read_csv("data-folder/autos.csv", encoding = "ISO-8859-1")
df_autos.head(2)

df_autos.columns

df_autos.dtypes

df_autos.shape

df_autos.model.head()

import numpy as np
np.sum(df_autos.model == 'golf')

df_autos[df_autos.model == 'golf'].head(3)

"""# Exercise 2 - Data cleaning"""

df_autos.describe(include='all')

"""## Price"""

# Exploring price distribution
df_autos.price.plot.box()

"""It does not seems to be normal at all. Big outliers. The price is assumed to be in EUR, so 1E9 makes no sense!"""

# Let us try some filter in prices
idx = (df_autos.price > 500) & (df_autos.price < 50e3)
df_autos[idx].price.plot.box()

idx = (df_autos.price > 500) & (df_autos.price < 15e3)
df_autos[idx].price.plot.box()

# Let us having a look at the histogram
df_autos[idx].price.plot.hist()

shape_filtered_df = df_autos[idx].shape
shape_full_df = df_autos.shape
"%d vs %d (%2.2f%%)"%(shape_filtered_df[0], shape_full_df[0], shape_filtered_df[0]/shape_full_df[0])

idx = (df_autos.price > 500) & (df_autos.price < 15e3)
df_autos = df_autos[idx]

"""## YearOfRegistration"""

df_autos.yearOfRegistration.plot.box()

idx = df_autos.yearOfRegistration <= 2019
df_autos[idx].yearOfRegistration.plot.box()

idx = (df_autos.yearOfRegistration <= 2019) & (df_autos.yearOfRegistration > 1950)
df_autos[idx].yearOfRegistration.plot.box()

idx = (df_autos.yearOfRegistration <= 2019) & (df_autos.yearOfRegistration > 1985)
df_autos[idx].yearOfRegistration.plot.box()

idx = np.abs(df_autos.yearOfRegistration - df_autos.yearOfRegistration.mean()) < 1 * df_autos.yearOfRegistration.std()
df_autos[idx].yearOfRegistration.plot.box()

shape_filtered_df = df_autos[idx].shape
shape_full_df = df_autos.shape
"%d vs %d (%2.2f%%)"%(shape_filtered_df[0], shape_full_df[0], shape_filtered_df[0]/shape_full_df[0]*100)

# Filtering out garbage in yearOfRegistration
df_autos = df_autos[idx]

"""# Explore the data"""

from pandas.plotting import scatter_matrix
_ = scatter_matrix(df_autos[['kilometer','price', 'yearOfRegistration']])

df_autos.loc[df_autos.model=='golf',['price', 'kilometer', 'model', 'yearOfRegistration']].groupby('model').describe().sort_values(by=('price','count'), ascending=False).T

"""# Linear Regression"""

from sklearn.linear_model import LinearRegression
PREDICTORS = ['kilometer', 'yearOfRegistration']
X = df_autos[PREDICTORS].values
y = df_autos.price.values
reg = LinearRegression().fit(X, y)

reg.score(X,y), reg.coef_

df_autos['prediction_lr'] = reg.predict(X)

df_autos[PREDICTORS + ['price', 'prediction_lr']].head()

df_autos['error_lr'] = df_autos.prediction_lr - df_autos.price
df_autos['abserror_lr'] = df_autos.error_lr.abs()

1- (np.sum(df_autos.error_lr**2) / np.sum((df_autos.price - df_autos.price.mean())**2))

from sklearn.metrics import *
print('ME', df_autos.error_lr.mean())
print('MAE', mean_absolute_error(y_true = df_autos.price, y_pred = df_autos.prediction_lr))
print('MSE', mean_squared_error(y_true = df_autos.price, y_pred = df_autos.prediction_lr))

df_autos.error_lr.plot.hist()

df_autos.groupby('model')[['abserror_lr']].mean().sort_values('abserror_lr', ascending=True).plot()

df_autos.groupby('model')[['abserror_lr']].agg(['mean', 'count']).sort_values(('abserror_lr','mean'), ascending=True).plot()

df_autos.groupby('model')[['abserror_lr']].agg(['mean', 'count']).abs().sort_values(('abserror_lr','mean'), ascending=True).head()

ax = df_autos[df_autos.model=='materia'].plot.scatter(x='kilometer', y='price')
df_autos[df_autos.model=='materia'].plot.scatter(x='kilometer', y='prediction_lr', ax=ax, color='orange')
import matplotlib.pyplot as plt

ax = df_autos[df_autos.model=='materia'].plot.scatter(x='yearOfRegistration', y='price')
df_autos[df_autos.model=='materia'].plot.scatter(x='yearOfRegistration', y='prediction_lr', ax=ax, color='orange')

ax = df_autos[df_autos.model=='a2'].plot.scatter(x='kilometer', y='price')
df_autos[df_autos.model=='a2'].plot.scatter(x='kilometer', y='prediction_lr', ax=ax, color='orange')

"""# Data normalization"""

from sklearn.preprocessing import QuantileTransformer
qt = QuantileTransformer()
X_norm = qt.fit_transform(X)

np.hstack((X, X_norm))[:2,:]

reg_l_norm = LinearRegression().fit(X_norm, y)
print(reg_l_norm.score(X_norm,y), reg_l_norm.coef_)

df_autos['prediction_lr_norm'] = reg_l_norm.predict(X_norm)
df_autos['error_lr_norm'] = df_autos.prediction_lr_norm - df_autos.price
df_autos['abserror_lr_norm'] = df_autos.error_lr_norm.abs()

print('ME', df_autos.error_lr_norm.mean())
print('MAE', mean_absolute_error(y_true = df_autos.price, y_pred = df_autos.prediction_lr_norm))
print('MSE', mean_squared_error(y_true = df_autos.price, y_pred = df_autos.prediction_lr_norm))

"""# Non Linear Regression"""

from sklearn.preprocessing import PolynomialFeatures
poly = PolynomialFeatures(3)
X_poly = poly.fit_transform(X_norm)

X_poly[0,:]

reg_nl = LinearRegression().fit(X_poly, y)
reg_nl.score(X_poly,y), reg_nl.coef_

df_autos['prediction_polr_norm'] = reg_nl.predict(X_poly)

df_autos['error_polr_norm'] = df_autos.prediction_polr_norm - df_autos.price
df_autos['abserror_polr_norm'] = df_autos.error_polr_norm.abs()

print('ME', df_autos.error_polr_norm.mean())
print('MAE', mean_absolute_error(y_true = df_autos.price, 
                                 y_pred = df_autos.prediction_polr_norm))
print('MSE', mean_squared_error(y_true = df_autos.price, 
                                y_pred = df_autos.prediction_polr_norm))

"""# Random Forest Regression"""

from sklearn.ensemble import RandomForestRegressor
from sklearn.pipeline import Pipeline
from sklearn.model_selection import GridSearchCV
from sklearn.preprocessing import normalize


regr = RandomForestRegressor(max_depth=5, random_state=0, n_estimators=100,
                            min_samples_leaf=5)

PREDICTORS = ['kilometer', 'yearOfRegistration']
X = df_autos[PREDICTORS].values
y = df_autos.price.values

regr.fit(X, y)

df_autos['prediction_rf'] = regr.predict(X)
df_autos['error_rf'] = df_autos.prediction_rf - df_autos.price
df_autos['abserror_rf'] = df_autos.error_rf.abs()

print('ME', df_autos.error_rf.mean())
print('MAE', mean_absolute_error(y_true = df_autos.price, 
                                 y_pred = df_autos.prediction_rf))
print('MSE', mean_squared_error(y_true = df_autos.price, 
                                y_pred = df_autos.prediction_rf))

"""## Adding categorical variables"""

!pip install category_encoders

import category_encoders as ce

PREDICTORS = ['kilometer', 'yearOfRegistration', 'model', 'gearbox', 'powerPS']
X = df_autos[PREDICTORS].values
y = df_autos.price.values
encoder = ce.TargetEncoder()
encoder.fit(X, y)

X_enc = encoder.transform(X)
regr = RandomForestRegressor(max_depth=5, random_state=0, n_estimators=300,
                            min_samples_leaf=5)

regr.fit(X_enc, y)
df_autos['prediction_rf'] = regr.predict(X_enc)
df_autos['error_rf'] = df_autos.prediction_rf - df_autos.price
df_autos['abserror_rf'] = df_autos.error_rf.abs()

print('ME', df_autos.error_rf.mean())
print('MAE', mean_absolute_error(y_true = df_autos.price, 
                                 y_pred = df_autos.prediction_rf))
print('MSE', mean_squared_error(y_true = df_autos.price, 
                                y_pred = df_autos.prediction_rf))

regr.feature_importances_

"""# Train - Test Split"""

from sklearn.model_selection import train_test_split
train, test = train_test_split(df_autos)

train.shape, test.shape

"""# Cross Validation"""

from sklearn.model_selection import GridSearchCV

# Set the parameters by cross-validation
tuned_parameters = [{'max_depth': [2,3,4],
                    'min_samples_leaf': [5,10]}]

clf = GridSearchCV(
    RandomForestRegressor(n_estimators=10), 
    tuned_parameters, cv=5, scoring='neg_mean_absolute_error')


PREDICTORS = ['kilometer', 'yearOfRegistration']
X = train[PREDICTORS].values
y = train.price.values


clf.fit(X, y)

clf.best_score_

clf.cv_results_

clf.score(test[PREDICTORS], test.price.values)

"""### Introducing pipelines
- Consider introducing categorical variables to this train-test CV setup
- We need to compute categorical mapping for each CV loop
- Otherwise we will have wrong results (validation error under estimation)

# Pipelines
"""

from sklearn.pipeline import Pipeline

pipe = Pipeline(steps=[('encoder', ce.TargetEncoder() ),
                       ('rf', RandomForestRegressor(n_estimators=10))])
# Set the parameters by cross-validation
tuned_parameters = [{'rf__max_depth': [2,3,4],
                    'rf__min_samples_leaf': [5,10]}]

clf = GridSearchCV(
    pipe, 
    tuned_parameters, cv=5, scoring='neg_mean_absolute_error',
    error_score=0
)

PREDICTORS = ['kilometer', 'yearOfRegistration', 'model', 'gearbox', 'powerPS']
X = train[PREDICTORS].values
y = train.price.values


clf.fit(X, y)

clf.best_score_

clf.best_estimator_

clf.score(test[PREDICTORS].values, test.price.values)

